\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\newcommand{\footlabel}[2]{%
	\addtocounter{footnote}{1}%
	\footnotetext[\thefootnote]{%
		\addtocounter{footnote}{-1}%
		\refstepcounter{footnote}\label{#1}%
		#2%
	}%
	$^{\ref{#1}}$%
}

\newcommand{\footref}[1]{%
	$^{\ref{#1}}$%
}

\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 3}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Università della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 3: Long Short-Term Memory Network}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	The goal of this project is to implement a text generator based on Long 
	Short-Term Memory (LSTM).
	
	
	
	\section{Preprocessing}
	First of all, the book \textit{The Count of Monte Cristo} has been  
	downloaded in plain English text from Project Gutenberg.
	
	The first operation performed consists in an initial preprocessing and 
	analysis of the data.
	First of all, all the text characters have been converted to lower case. 
	Soon after, the number of unique characters were counted as well as the 
	absolute and relative frequency of each character. In total there are $106$ 
	unique characters in the book.
	
	In order to train the network, it is necessary to map the text strings to a 
	numerical representation. Hence, two dictionaries that act as lookup tables 
	are created: one mapping characters to integer, and the other integers to 
	characters. This will be crucial for the creation of a one-hot encoding.
	
	In Table \ref{tab:statistics} are presented the $30$ most frequent 
	characters, with the corresponding encoding and frequency.
	
	\begin{figure}[H]
	\centering
	\begin{tabular}{llcc}
		\toprule
		Encoding & Character &  Absolute Frequence & Relative Frequence \\
		\midrule
		8        &           &              420940 &            15.901\% \\
		5        &         e &              259094 &             9.787\% \\
		7        &         t &              180542 &              6.82\% \\
		21       &         a &              165546 &             6.253\% \\
		3        &         o &              157076 &             5.933\% \\
		18       &         i &              142302 &             5.375\% \\
		11       &         n &              137584 &             5.197\% \\
		14       &         s &              126590 &             4.782\% \\
		15       &         h &              126368 &             4.773\% \\
		2        &         r &              121407 &             4.586\% \\
		24       &         d &               94099 &             3.554\% \\
		22       &         l &               80730 &             3.049\% \\
		0        &        \textbackslash n &               61739 &             
		2.332\% \\
		10       &         u &               60318 &             2.278\% \\
		17       &         m &               57157 &             2.159\% \\
		6        &         c &               52526 &             1.984\% \\
		16       &         f &               45383 &             1.714\% \\
		19       &         , &               45246 &             1.709\% \\
		27       &         w &               43892 &             1.658\% \\
		20       &         y &               42642 &             1.611\% \\
		9        &         g &               35603 &             1.345\% \\
		1        &         p &               34952 &              1.32\% \\
		12       &         b &               27972 &             1.057\% \\
		29       &         . &               21950 &             0.829\% \\
		28       &         v &               21490 &             0.812\% \\
		55       &         “ &               15712 &             0.594\% \\
		57       &         ” &               15195 &             0.574\% \\
		26       &         k &               12649 &             0.478\% \\
		54       &         ; &                5993 &             0.226\% \\
		56       &         ? &                4610 &             0.174\% \\
		\dots       &         \dots &                \dots &             \dots 
		\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Books Statistics.}
	\label{tab:statistics}
	\end{figure}

	
	The next step was the creation of a dictionary in which each integer 
	represent each character. 
	
	
	\section{Truncated backpropagation through time}
	
	\section{Evolution of the training loss function}
	
	\section{Generate and document 20 sequences}
	
	\section{Improvement of the model}
	In the first model trained, also blanks and other noisy characters have 
	been considered. It would be interesting to document the evolution of the 
	training loss function of a model with a different preprocessing of data.
	
	%FIXME
	In particular, could be interesting removing 
	(think about removal of punctuation etc…)
	greek sentence
	
	This kind of preprocessing of the text can be useful to generate more
	interesting text 
	
	
	\section{New network implementation}
	
	

\end{document}
