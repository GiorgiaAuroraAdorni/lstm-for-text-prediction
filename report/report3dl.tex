\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\newcommand{\footlabel}[2]{%
	\addtocounter{footnote}{1}%
	\footnotetext[\thefootnote]{%
		\addtocounter{footnote}{-1}%
		\refstepcounter{footnote}\label{#1}%
		#2%
	}%
	$^{\ref{#1}}$%
}

\newcommand{\footref}[1]{%
	$^{\ref{#1}}$%
}

\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 3}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Università della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 3: Long Short-Term Memory Network}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	\label{section:intro}
	The goal of this project is to implement a text generator based on Long 
	Short-Term Memory (LSTM).
	
	\section{Preprocessing}
	\label{section:preprocessing}
	First of all, the book \textit{The Count of Monte Cristo} has been  
	downloaded in plain English text from Project Gutenberg.
	
	The first operation performed is the application of an initial 
	preprocessing procedure that consists in the conversion to lower case of 
	all the text characters. Afterwards, a simple analysis of the data was 
	performed: the number of unique characters were counted as well as the 
	absolute and relative frequency of each character. In total there are $106$ 
	unique characters in the book.\bigskip
	
	In order to train the network, it is necessary to map the text strings to a 
	numerical representation. Hence, two dictionaries that act as lookup tables 
	are created: one mapping characters to integer, and the other integers to 
	characters. This will be crucial for the creation of a one-hot encoding.
	
	In Table \ref{tab:statistics} are presented the $30$ most frequent 
	characters, with the corresponding encoding and frequency.
	
	\begin{figure}[htb]
	\centering
	\begin{tabular}{llcc}
		\toprule
		Encoding & Characters &  Absolute Frequencies & Relative Frequencies \\
		\midrule
		8        &           &              420940 &            15.901\% \\
		5        &         e &              259094 &             9.787\% \\
		7        &         t &              180542 &              6.82\% \\
		21       &         a &              165546 &             6.253\% \\
		3        &         o &              157076 &             5.933\% \\
		18       &         i &              142302 &             5.375\% \\
		11       &         n &              137584 &             5.197\% \\
		14       &         s &              126590 &             4.782\% \\
		15       &         h &              126368 &             4.773\% \\
		2        &         r &              121407 &             4.586\% \\
		24       &         d &               94099 &             3.554\% \\
		22       &         l &               80730 &             3.049\% \\
		0        &        \textbackslash n &               61739 &             
		2.332\% \\
		10       &         u &               60318 &             2.278\% \\
		17       &         m &               57157 &             2.159\% \\
		6        &         c &               52526 &             1.984\% \\
		16       &         f &               45383 &             1.714\% \\
		19       &         , &               45246 &             1.709\% \\
		27       &         w &               43892 &             1.658\% \\
		20       &         y &               42642 &             1.611\% \\
		9        &         g &               35603 &             1.345\% \\
		1        &         p &               34952 &              1.32\% \\
		12       &         b &               27972 &             1.057\% \\
		29       &         . &               21950 &             0.829\% \\
		28       &         v &               21490 &             0.812\% \\
		55       &         “ &               15712 &             0.594\% \\
		57       &         ” &               15195 &             0.574\% \\
		26       &         k &               12649 &             0.478\% \\
		54       &         ; &                5993 &             0.226\% \\
		56       &         ? &                4610 &             0.174\% \\
		\dots       &         \dots &                \dots &             \dots 
		\\
		\bottomrule
	\end{tabular}
	\captionof{table}{Encoding and frequency of the 30 most frequent 
	characters}
	\label{tab:statistics}
	\end{figure}

	\bigskip
	Soon after, training examples and targets were created. The input examples 
	corresponds to a sequence of characters, while the target of each character 
	corresponds to the following character in the original sequence. In this 
	way, there is no target for the last element of the sequence.
	
	\section{Truncated backpropagation through time}
	\label{section:backpropagation}
	
	Given a very long sequence, it is impossible to feeding it entirely and 
	then compute the back-propagation. Instead, it is possible to divide the 
	sequence in $n$ blocks and each block in $m$ subsequences which contain a 
	fixed number of characters of the text.\bigskip
	
	The $i-\mathrm{th}$ batch is created taking the $i-\mathrm{th}$ subsequence 
	from each block and computing the back-propagation on it. In this way, each 
	batch has multiple subsequences and the computations are more efficient.
	\textbf{FIXME:}This approach enables to pass the output state from a 
	sequence to the one of the next block. The zero state is at the beginning 
	of each epoch. \textbf{In TensorFlow, this requires feeding the current 
	state to initial state, fetching final state, and feeding it to initial 
	state in the next iteration.}\bigskip
	
	In the experiment that will be presented have been used 16 blocks with 
	subsequences of size 256. 
	To ensure that all batches have sequences of the same length, before 
	crating the batches, the text is padded in such a way that sequences that 
	are shorter than 256 are filled with 0 at the end, thus avoiding to 
	truncate them.
	
	A mask containing the index of the valid characters is created in order to 
	be use to computed the loss.
	
	\section{Network}
	\label{section:network}
	
	The model implemented is composed of a MultiRNNCell with two LSTMCells each 
	containing 256 units, following by a softmax output layer with $k$ units, 
	which correspond to the number of unique characters (one-hot encoding), in 
	this case 106.
	
	In Table \ref{tab:model0} is summarised the architecture of the network 
	used in the first experiment.	
	\begin{figure}[H]
		\centering
		
		\begin{tabular}{ccc}
			\toprule
			\textbf{LSTMCell1} & \textbf{LSTMCell2} & \textbf{softmax} \\
			\midrule
			256 & 256 & 106\\
			\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:model0}
	\end{figure}

	\section{Evolution of the training loss function}
	\label{section:loss}

	The training would take 5 epochs and Adam is used as optimiser with a 
	learning rate of $10^2$. As loss function, the Softmax Cross Entropy with 
	Logits is used since the problem can be treated as a classification one.
	
	All the models were implemented using TensorFlow and trained on an NVIDIA 
	Tesla V100-PCIE-16GB GPU.
	
	The training loss of this experiment is shown in Figure 
	\ref{fig:model0-loss}.
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../src/out/initial/img/loss.pdf}	
		\caption{Training loss}
		\label{fig:model0-loss}
	\end{figure}

	\section{Generate and document 20 sequences}
	After training, in order to evaluate the network, 20 sequences composed of 
	256 characters are generated.\bigskip
	
	The procedure of generation of a sequence starts by choosing an initial 
	character randomly, based on the relative frequencies presented in Section 
	\ref{section:preprocessing}.
	
	After that, the prediction distribution of the next character is given 
	using the initial character and the state of the network. In order to 
	calculate the index of the predicted character, a categorical distribution 
	is used. 
	
	The predicted character is used as the following input of the model along 
	with the previous hidden state.
	
	\section{Improvement of the model}
	In the first model trained, also blanks and other noisy characters have 
	been considered. It would be interesting to document the evolution of the 
	training loss function of a model with a different preprocessing of data.
	
	%FIXME
	In particular, could be interesting removing 
	(think about removal of punctuation etc…)
	greek sentence
	
	This kind of preprocessing of the text can be useful to generate more
	interesting text 
	
	
	\section{New network implementation}
	
	

\end{document}
